{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Installing Requests library\n",
    "#Requests is a simple and elegant Python HTTP library which provides methods for accessing Web resources via HTTP.\n",
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1277 sha256=34f7c792afeeb08c5026d4e72a2214b17f2332cb85495f743a5157f511e66653\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\75\\78\\21\\68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Installing beautifulsoup [Python library] for pulling data out of HTML and XML files\n",
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the username you wish to scrape: shikha12264\n",
      "Profile Link: https://github.com/shikha12264\n"
     ]
    }
   ],
   "source": [
    "import requests # importing requests libraray\n",
    "from bs4 import BeautifulSoup as bs #importing beautifulsoup from bs4 libraray\n",
    "user=input(\"Enter the username you wish to scrape: \")\n",
    "\n",
    "# for scraping first we need to send a request to the particular url and then get a response back with 200 response code  \n",
    "# getting all the output in html format [html parser]\n",
    "# send request\n",
    "\n",
    "url=\"https://github.com/\"+user #string concatenation to conacetatenate username to the link\n",
    "print(\"Profile Link:\",url) #printing the appropriate user link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link for profile image: https://avatars.githubusercontent.com/u/64529469?v=4\n"
     ]
    }
   ],
   "source": [
    "r= requests.get(url) # requests.get is a method of requests library where you pass the url to scrape the website\n",
    "# The get() method issues a GET request; it fetches documents identified by the given URL.\n",
    "\n",
    "#creating a BeautifulSoup object i.e. soup object or a parse tree!\n",
    "#Iterating through all HTML tags using Pythonâ€™s built-in html parser.\n",
    "soup= bs(r.content,'html.parser')\n",
    "\n",
    "#Fetching img tag with src i.e. the 'source attribute'.\n",
    "profile_image= soup.find('img',{'class':'avatar avatar-user width-full border color-bg-primary'})['src']\n",
    "print(\"Link for profile image:\",profile_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
